vl_abliteration is a GUI-based tool designed to abliterat(e) vision-language and text-only transformer models in a controlled, research-friendly way. Built with Gradio, it lets users load Qwen-family VL models or standard causal LLMs, compute a refusal direction by contrasting harmful and harmless instructions, and then neutralize that direction by orthogonalizing key attention and MLP projection weights across selected layers. The app supports flexible layer-depth targeting, optional 4-bit loading for memory efficiency, and end-to-end workflows including local saving and direct upload to Hugging Face. With robust model and layer detection, automatic processor handling, and real-time logs, vl_abliteration makes experimenting with safety, alignment, and behavior steering in multimodal transformer models accessible through a clean, interactive interface.
